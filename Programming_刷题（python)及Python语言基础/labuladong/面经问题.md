## 面经问题

1. 特征值和特征向量的含义？

   $A\hat{v} = \lambda \hat{v}$, 表示的其实是原空间向量经过线性变换后到新空间向量的方向不变性，$\lambda$为伸缩的范围；

   线性变换实际上就是对向量进行旋转或者伸缩变换

   reference：https://www.zhihu.com/question/21874816

   

2. 深度学习 学习曲线震荡的原因？

   https://blog.csdn.net/fu6543210/article/details/89790220

   a. batch_size太小

   b. 数据输入不合适

   c. model路径

   

3.  逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？

   如果在损失函数最终收敛的情况下，有很多特征高度相关也不会影响分类器的效果。对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。
   
   **LR为什么使用sigmoid作为激活函数，其他的不行吗?**
   
   首先用一句话来概括LR： **LR假设数据符合伯努利分布，通过极大化似然函数的方法，通过梯度下降求解参数， 达到对数据进行二分类的目的。**当数据为伯努利分布时，其指数族就是sigmoid函数
   
   
   
   
   
4. 梯度下降的方法？优缺点？

   梯度下降主要有BGD、SGD、MBGD三种方法。

   BGD会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。

   SGD以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。

   MBGD降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中可以采用这种方法，将数据一个batch一个batch的送进去训练。

   

5. 上述梯度下降的方法中有两个问题不可避免（1）learning rate如何选取? (2)对每个参数采用一样的学习率？

   $$w^{t} = w^{t-1} - l_r * m_t/\sqrt{v_t}$$

   $m_t$:一阶动量， 和梯度相关的函数

   $v_t$: 二阶动量， 和梯度平方相关的函数 

   learning rate 由不同的一阶动量和二阶动量共同决定

   (1) SGD： $m_t = g_t v_t = 1$

   (2) SGDM:SGD基础上加上一阶动量的滑动平均

    $m_t = \beta m_{t-1} + (1-\beta) g_t v_t=1$. $m_t$为各个时刻梯度方向的指数滑动平均值

   (3) Adagrad：SGD基础上加上二阶动量

   $m_t = g_t v_t = \sum_{\tau=1}^{t}g_\tau^2$ 给每个参数分配自适应学习率

   (4) RMSprop: 再SGD上加上二阶动量的滑动平均

   $m_t = g_t v_t = \beta v_{t-1} + (1-\beta)g_t^2$ 

   (4) Adam: SGDM的一阶动量和RMSprob的二阶动量加上修正值

   $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$ $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$

   修正后

   $\hat{m_t} =  \frac {m_t} {1-\beta_1^t}$

   $\hat{V_t} =  \frac {V_t} {1-\beta_2^t}$

   

6. 不平衡数据集常用的评价指标？处理方法？

   评价指标：(1)混淆矩阵相关 precission, recall, f1_score；（2）ROC, AUC

   处理方法：[数据不平衡—采样](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L1)

   （1）数据集增强（2）采样：重采样，欠采样（大类数据）

   

7. Pearson系数和Spearman系数的区别？

   a. 定义不同：

   Pearson相关系数被定义为他们的协方差除以标准差的乘积；Spearman相关性系数被定义为秩（有序）变量zhi之间的Pearson相关系数。

   b. 线性不同：

   pearson相关系数时线性相关关系， spearman相关性系数呈现非线性相关

   c. 连续性不同:

   pearson相关系数呈现**连续型正态分布变量**之间的线性关系，spearman不要求正态连续，至少是有序；

   d. 使用场景不同：

   pearson相关是最常见的相关公式，用于计算连续数据的相关，比如计算班上学生数学成绩和语文成绩的相关可以用Pearson相关。

   而spearman相关是专门用于分析顺序数据的，就是那种只有顺序关系，但并非等距的数据，比如计算班上学生数学成绩排名和语文成绩排名的关系。

   

8. 

   

​    