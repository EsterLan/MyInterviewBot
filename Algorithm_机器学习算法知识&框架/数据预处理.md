## 数据预处理EDA

refer to [IBM Developer数据预处理](https://developer.ibm.com/zh/articles/data-preprocessing-in-detail/#)

### 1. 数据清理（缺失值，异常值）

#### 缺失值

1. **移除训练示例（样本、特征）**：若缺失为标签，忽略该样本；若为特征，通常 不采用该方法来处理，因为会导致数据丢失，移除的属性值也有信息。
2. **手动填充**: 耗时。
3. **使用标准值来替换缺失值**：缺失值可替换为全局变量缺失值可替换为全局常量（例如“N/A”或“Unknown”）这是一种简单方法，但并非万无一失。
4. **使用属性的集中趋势（平均值、中间值、众数）来替换缺失值**：根据数据分布，可使用平均值（适用于正态分布）或中间值（适用于非正态分布）来填充缺失值。
5. **使用同类属性的集中趋势（平均值、中间值、众数）来替换缺失值**：此方法与方法 4 相同，但集中趋势的度量值因每个类而异。
6. **使用最可能的值来填充缺失值**：可使用回归和决策树等算法来预测并替换缺失值。

1.4.5.6常使用

#### 干扰数据

通过变量的随机方差来定义干扰（类别）。数字值通过箱线图和散点图识别。处理异常值的方式：数据平滑技术

- **分箱**：可使用分箱方法，利用排序值周围的值来对该排序值进行平滑处理。这样，排序值便可以分为多个“箱”。有多种方法可用于分箱。其中有一种方法采用箱平均值进行平滑处理，即把每个箱替换为该箱中的值的平均值，还有一种方法采用箱中间值进行平滑处理，即把每个箱替换为该箱中的值的中间值
- **回归**：可使用线性回归和多重线性回归来对数据进行平滑处理，其中所有值都符合某一函数。
- **异常值分析**：可使用分群等方法来检测和处理异常值。

### 2. 数据集成（Hadoop？）

由于数据是从多种来源收集而来，因此“数据集成”已经成为数据预处理流程中的一个重要组成部分。这可能导致出现冗余数据和不一致数据，从而导致数据模型的准确性和速度都有所下降。为解决此类问题并保持数据完整性，随后需要使用诸如元组重复检测和数据冲突检测等方法。下面解释了最常用的数据集成方法。

1. **数据整合**：以物理方式将数据一起导入到同一个数据存储。这通常涉及数据仓储技术。
2. **数据传播**：使用称为“数据传播”的应用程序将数据从一个位置复制到另一个位置。此过程可同步或异步执行，并且属于事件驱动型操作。
3. **数据虚拟化**：使用界面提供来自多种不同来源的数据的实时的统一视图。可从单一访问点来查看数据。

### 3. 数据缩减（特征工程：特征选择）

“数据缩减”是为了以精简方式表示数据集，这样得到的数据量较小，同时又保留了原始数据的完整性。这样可得到高效且相似的结果。减少数据量的方法包括：

1. **缺失值比率**：移除所含缺失值数量超过阈值的属性。
2. **低方差过滤器**：也会移除其方差（分布）小于阈值的规范化属性，因为数据变化较少就意味着信息较少。
3. **高相关过滤器**：也会移除其相关系数超过阈值的规范化属性，因为相似的趋势意味着携带相似的信息。通常使用 Pearson 卡方值等统计方法来计算相关系数。
4. **主成份分析**：主成份分析 (PCA) 是一种统计方法，它通过将高度相关的属性集中在一起来减少属性数量。通过每次迭代，初始特征缩减为主成份，并且其方差大于原始集合，前提是这些主成份与原先成份不相关。但此方法仅适用于具有数字值的特征。

### 4. 数据转换（编码，归一化等）

数据预处理流程的最后一个步骤是将数据转换为适合数据建模的格式。支持数据转换的策略包括：

1. **平滑处理**
2. **属性/特征构造**：通过给定属性集来构造新属性。
3. **聚合**：针对给定属性集应用“摘要”和“聚合”操作来构造新属性。
4. **规范化**：每个属性中的数据都会缩放到较小的范围（例如，0 到 1 或 -1 到 1）内。
5. **离散化**：数字属性的原始值将替换为离散或概念区间，以便进一步组织为更高级的区间。
6. **名义数据的概念层次结构生成**：名义数据值将归纳为更高阶的概念。

### 5. 数据不均衡的问题

数据不均衡的处理方法主要包括

1. Sampling方法：基于采样的方法，对数据集进行过采样或者欠采样。欠采样会带来信息丢失的问题，过采样又会导致过拟合问题。还有一种采样是插值方法，如smote(选择每个正样本的k近邻，然后在该样本和近邻样本的连线上随机采样)，对数据进行重新生成的方法。
2. 采用多个分组将多数的类别数据拆分成多个少数组数据，然后分别进行训练取bagging。这个方法更适合于集成方法
3. kernel类模型，还可以通过修改核函数偏移超平面，抵消不均衡问题。
4. 通过修改cost function的计算方法，对少数类的损失赋予一个较大的权值，让模型更注重少数类的学习。 

### 常问

refer to:

 [机器学习项目（主数据处理)](https://gjwei.github.io/2018/06/02/机器学习面试/)

- 数据处理：如何处理缺失数据(missing value)? 各种处理方法有什么利弊？

  > (1)用平均数、中值、分位数、众数、随机数填充。效果一般，等价认为增加噪声。
  >
  > (2)用模型算法预测。效果好于（1），缺陷在于，若预测不准，预测结果没有意义；预测很准，说明特征与其他特征相关性大，没有必要。
  >
  > (3)最精确的做法：将变量映射到高维空间。比如性别有男、女和缺失三种情况，则可以映射为是否男，是否女，是否缺失（one hot形式）。连续变量也可以这样处理。不需要考虑缺失值，保留了完整的数据信息。
  > 但是计算量会大大增加。

- 数据处理：如何将描述变量(categorical variables)转为连续变量(continuous variables)?

  - 如何处理有序变量？

  - 如何处理无序变量？

  > 对于无序变量，oneHot编码；对于有序变量，可以将其转换为无序变量进行编码，但是不应该忽视其取值之间的关联性，如good normal bad， bad<normal<good
  >
  > ```python
  > bad	(1, 0, 0) 
  > normal	(1, 1, 0)
  > good	(1, 1, 1)
  > ```

- 数据处理：如何进行选择特征选择？如何进行数据压缩？

  - 特征选择：包裹式，过滤式，嵌入式

  - 数据压缩：主成分分析，自编码等

  

### 工程实践（1）From the perspective of Sklearn

#### preprocessing

- 无量纲化

> 好处：
>
> （1）归一化后加快了梯度下降求最优解的速度。
>
> 梯度下降“小步慢走”，通通过不断的迭代，求得最优解，归一化让特征向量中不同特征的取值相差不大，训练速度大大加快。
>
> （2）归一化有可能提高精度。
>
> 计算样本距离时，如果特征向量取值范围相差很大，不进行归一化处理，则值范围更大的特征向量对距离的影响更大，实际情况是，取值范围更小的特征向量对距离影响更大，这样的话，精度就会收到影响。

- 适用模型

  从上述可知：什么样的模型的数据需要无量纲化。梯度下降优化的：如LR, NN等；还有基于“距离”的模型：如svm，knn，各种聚类模型pca，lda

  如决策树类模型就不需要归一化。

- 主要类型

（1）min-max归一化

<img src="C:\Users\Ester.L\AppData\Roaming\Typora\typora-user-images\image-20200722214237562.png" alt="image-20200722214237562" style="zoom:67%;" />

（2）z-score标准化

<img src="C:\Users\Ester.L\AppData\Roaming\Typora\typora-user-images\image-20200722214319344.png" alt="image-20200722214319344" style="zoom:67%;" />

- 适用情景：

  - 上式中，min是样本的最小值，max是样本的最大值。由于最大值与最小值可能是动态变化的，同时也**非常容易受噪声(异常点、离群点)影响**，因此一般**适合小数据**的场景。此外，该方法还有两点好处：

       1) 如果某属性/特征的方差很小，如身高：np.array([[1.70],[1.71],[1.72],[1.70],[1.73]])，实际5条数据在身高这个特征上是有差异的，但是却很微弱，这样不利于模型的学习，进行min-max归一化后为：array([[ 0. ], [ 0.33333333], [ 0.66666667], [ 0. ], [ 1. ]])，相当于放大了差异；

       2) 维持稀疏矩阵中为0的条目。

  - 于是，可以看出，z-score标准化方法试图将原始数据集标准化成均值为0，方差为1且接近于标准正态分布的数据集。然而，一旦原始数据的分布 不 接近于一般正态分布，则标准化的效果会不好。**该方法比较适合数据量大的场景**(即样本足够多，现在都流行大数据，因此可以比较放心地用)。此外，<u>相对于min-max归一化方法，该方法不仅能够去除量纲，还能够把所有维度的变量一视同仁(因为每个维度都服从均值为0、方差1的正态分布)，在最后计算距离时各个维度数据发挥了相同的作用，避免了不同量纲的选取对距离计算产生的巨大影响。所以，涉及到计算点与点之间的距离，如利用距离度量来计算相似度、PCA、LDA，聚类分析等，并且数据量大(近似正态分布)，可考虑该方法。相反地，如果想保留原始数据中由标准差所反映的潜在权重关系应该选择min-max归一化</u>
  **[reference]**原文链接：https://blog.csdn.net/OnTheWayGoGoing/java/article/details/79871559


#### 缺失值处理impute

  SimpleInputer(missing_values)

  fillna

  dropna

#### 处理categorical 变量

（1）对于标签：LabelEncoder，仅允许一维输入

（2）对于特征：OridinalEncoder，输入>=2d。

```python
['highSchool', 'college', 'primary']->[0,1,2]
```



（3）OneHotEncoder：独热编码，创建哑变量

```python
['S', 'C', 'Q']->
[[1 0 0],[0 1 0], [0 0 1]]
```

**什么时候采用oneHot呢？**以下根据类型变量的实际意义对其进行分类

- 名义变量：如舱门（S， C, Q），取值之间相互独立，不能加减的变量。
- 有序变量：如学历（小学，高中），这类变量不完全独立，可排序但不能相互计算。
- 有距变量：年份等离散型数据可， 温度等连续型数据也可。

#### 处理连续型变量（后写）

（1）原始度量：用数值型变量来直接表示为特征，而不需要任何形式的变换或特征工程。通常这些特征可以表示一些值或总数。

> 数值：describe()获得基本统计量，原始数据型特征
>
> 计数：原始度量的另一种形式，包括频率、综述或特征发生次数特征。

（2）二值化

#### 特征选择、特征压缩feature_selection/decompostion

特征选择： filter， SelectFromModel, Wrapper

[feature_selection_practice](https://github.com/EsterLan/jupyter_note/blob/master/sklearn/特征选择.ipynb)

[feature_selection_theory](https://zhuanlan.zhihu.com/p/58331398)

[互信息、信息熵等](https://blog.csdn.net/pipisorry/article/details/51695283)

### 工程实践（2）

more refer to: Datawhale

[用pandas处理文本数据](https://mp.weixin.qq.com/s/Go7ZUEiItrdc_KOWEAc3sg)

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsENEJFj6PQduhmseMibu7SmZrGnTBOEPAWSsYzSzjGcRbb6CfnzmPCTnLqHdiaJaC7H8kfng3ibicEBUw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

[缺失数据处理](https://mp.weixin.qq.com/s/_wI_uvkDDE3-fhdb9oNHuA)

![image-20200724111851770](C:\Users\Ester.L\AppData\Roaming\Typora\typora-user-images\image-20200724111851770.png)



[pandas处理分类数据](https://mp.weixin.qq.com/s/gaEJAqW5H6ja7wAYgjJsAA)

![img](https://mmbiz.qpic.cn/mmbiz_png/vI9nYe94fsHXkd6YFkx0GEQNvVNPf729EkHOPY37psTIb7BQnraXUl5yJBTx2s20vshSicX11iabQrx6YEfsZFhA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



